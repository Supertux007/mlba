import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler
# We copy the training and testing datasets from R to Python.
carset_tr_py = r.df_tr.copy()
carset_te_py= r.df_te.copy()
# We encode categorical variables as numeric, which is necessary for CART.
le = OneHotEncoder()
cat_vars = ["ShelveLoc", "Urban", "US", "SaleHigh"]
for var in cat_vars:
# Fit the LabelEncoder to the training set and transform the training and testing sets
# We need to use the same encoder for both sets to ensure consistency
carset_tr_py_encoded = le.fit_transform(carset_tr_py[[var]].values.reshape(-1, 1))
carset_te_py_encoded = le.transform(carset_te_py[[var]].values.reshape(-1, 1))
carset_tr_py[var] = carset_tr_py_encoded.toarray()
carset_te_py[var] = carset_te_py_encoded.toarray()
# Split the data into training and testing sets
X_train, y_train = carset_tr_py.drop(columns=["SaleHigh"]), carset_tr_py["SaleHigh"]
X_test, y_test = carset_te_py.drop(columns=["SaleHigh"]), carset_te_py["SaleHigh"]
# Standardize only the continuous variables
cont_vars = ["CompPrice", "Income", "Advertising", "Population", "Price", "Age", "Education"]
scaler = StandardScaler()
X_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])
X_test[cont_vars] = scaler.transform(X_test[cont_vars])
# To speed up the operation you can also transform the inputs
# X_train = X_train.to_numpy()
# y_train = y_train.to_numpy()
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
# we clear any previous figures
plt.clf()
np.random.seed(1234)
carseats_tree = DecisionTreeClassifier().fit(X_train, y_train)
plt.figure(figsize=(20,10))
plot_tree(carseats_tree,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)
plt.savefig('tree_high_dpi', dpi=300)
# for a better quality, save the image and load it again
#plt.show()
# Get the paths of the leaf nodes for the Car Seats decision tree using cost complexity pruning
path = carseats_tree.cost_complexity_pruning_path(X_train, y_train)
# Extract the effective alphas and total impurities of the leaf nodes from the path object
ccp_alphas, impurities = path.ccp_alphas, path.impurities
# Create a plot to visualize the relationship between effective alphas and total impurities
fig, ax = plt.subplots()
ax.plot(ccp_alphas, impurities, marker='o', linestyle="-")
ax.set_xlabel("Effective alpha")
ax.set_ylabel("Total impurity of leaves")
ax.set_title("Total Impurity vs Effective alpha for Car Seats dataset")
ax.invert_xaxis()
plt.show()
from sklearn.model_selection import cross_val_score, KFold
def plotcp(X_train, y_train, random_state=123):
# Create a decision tree classifier
clf = DecisionTreeClassifier(random_state=random_state)
# Calculate the cross-validation scores for different values of alpha
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
# Perform cross-validation for each alpha
kfold = KFold(n_splits=10, shuffle=True, random_state=random_state)
scores = []
for ccp_alpha in ccp_alphas:
clf = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha)
score = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')
scores.append(np.mean(score))
# Plot the cross-validation scores vs alpha
fig, ax = plt.subplots()
ax.plot(ccp_alphas, scores, marker='o', linestyle="-")
ax.set_xlabel("ccp_alpha")
ax.set_ylabel("Cross-validation score (accuracy)")
ax.set_title("Pruning Complexity Parameter (ccp) vs Cross-validation Score")
ax.invert_xaxis()
plt.show()
plotcp(X_train, y_train)
# Create a decision tree classifier with a ccp_alpha of 0.025
carseats_tree_prune = DecisionTreeClassifier(random_state=123, ccp_alpha=0.008)
# Fit the model to the data
carseats_tree_prune.fit(X_train, y_train)
# You can again plot the figure with
plt.clf()
plt.figure(figsize=(12,10))
plot_tree(carseats_tree_prune,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)
plt.savefig('tree_pruned_high_dpi', dpi=200)
plt.close()
from sklearn.model_selection import GridSearchCV
# Set up the decision tree classifier
tree = DecisionTreeClassifier()
# Set up the grid search parameters
param_grid = {'max_depth': range(1, 11)}
# Run grid search cross-validation to find optimal tree depth
grid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)
grid_search.fit(X_train, y_train)
# Use the best estimator to fit and prune the tree
pruned_tree = grid_search.best_estimator_
plt.clf()
fig, ax = plt.subplots(figsize=(15, 10))
plot_tree(pruned_tree, ax=ax, feature_names=X_train.columns)
plt.show()
# you can choose to re-train the model once again with this new parameter
# Set up the decision tree classifier
tree = DecisionTreeClassifier()
# Set up the grid search parameters
param_grid = {'max_depth': range(1, 11)}
# Run grid search cross-validation to find optimal tree depth
grid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)
grid_search.fit(X_train, y_train)
# Calculate the mean and standard error of test scores for each tree depth
mean_scores = grid_search.cv_results_['mean_test_score']
std_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)
# Find the optimal depth using the 1-SE rule
optimal_depth = grid_search.best_params_['max_depth']
optimal_score = mean_scores[optimal_depth - 1]
se = std_scores[optimal_depth - 1]
best_depth = optimal_depth
for depth in range(optimal_depth - 1, -1, -1):
score = mean_scores[depth]
if score + se < optimal_score:
break
else:
best_depth = depth + 1
# Use the best estimator to fit and prune the tree
pruned_tree = DecisionTreeClassifier(max_depth=best_depth)
pruned_tree.fit(X_train, y_train)
# Set up the decision tree classifier
tree = DecisionTreeClassifier()
# Set up the grid search parameters
param_grid = {'max_depth': range(1, 11)}
# Run grid search cross-validation to find optimal tree depth
grid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)
grid_search.fit(X_train, y_train)
# Calculate the mean and standard error of test scores for each tree depth
mean_scores = grid_search.cv_results_['mean_test_score']
std_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)
# Find the optimal depth using the 1-SE rule
optimal_depth = grid_search.best_params_['max_depth']
optimal_score = mean_scores[optimal_depth - 1]
se = std_scores[optimal_depth - 1]
best_depth = optimal_depth
for depth in range(optimal_depth - 1, -1, -1):
score = mean_scores[depth]
if score + se < optimal_score:
break
else:
best_depth = depth + 1
# Use the best estimator to fit and prune the tree
pruned_tree = DecisionTreeClassifier(max_depth=best_depth)
pruned_tree.fit(X_train, y_train)
# Predict the class labels for the test data with the python implementation
y_pred = carseats_tree_prune.predict(X_test)
# Print a confusion matrix of the predicted labels versus the true labels
print(pd.crosstab(index=y_pred, columns=y_test, rownames=['Pred'], colnames=['Obs']))
# Predict the class probabilities for the test data
y_probs = carseats_tree_prune.predict_proba(X_test)
print(pd.DataFrame(y_probs))
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import OrdinalEncoder
# Select different columns for the regression task from the original carsets data
X, y = r.Carseats.drop(columns=["Sales"]), r.Carseats["Sales"]
# Once again select the categorical columns (this time without `SalesHigh`)
categorical_cols = ['ShelveLoc', 'Urban', 'US']
# Encode categorical columns as integers
encoder = OrdinalEncoder()
X[categorical_cols] = encoder.fit_transform(X[categorical_cols])
# Build the regression tree model
carseats_reg = DecisionTreeRegressor(random_state=123)
carseats_reg.fit(X, y)
# Visualize the tree (a bit messy)
# fig, ax = plt.subplots(figsize=(12, 8))
# plot_tree(carseats_reg, ax=ax, feature_names=X.columns)
# plt.show()
# Make predictions on the training data
carseat_reg_pred = carseats_reg.predict(X)
# Create a scatter plot of predicted versus true values
plt.clf()
fig, ax = plt.subplots(figsize=(8, 8))
ax.scatter(y, carseat_reg_pred, alpha=0.5)
ax.set_xlabel('Sales')
ax.set_ylabel('Predictions')
ax.plot(ax.get_xlim(), ax.get_ylim(), ls="--", c=".3")
plt.show()
